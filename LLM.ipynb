{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOjYB7Bo0oFyZ5iUR8zOMg2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["First, we need to change the environment to use the GPU:\n","\n","1. Entorno de ejecucción\n","2. Cambiar entorno de ejecución\n","3. Seleccionar T4 GPU\n","\n","\n","\n"],"metadata":{"id":"sx5CoQqoqxgr"}},{"cell_type":"markdown","source":["## Load dependencies"],"metadata":{"id":"EwrXM6Hird1-"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cjHkWXnSmk_C","outputId":"d516fcda-9f87-4fa6-f4da-b5526497362e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m794.4/794.4 kB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.2/37.2 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q -U torch datasets transformers tensorflow langchain playwright html2text sentence_transformers faiss-cpu streamlit\n","!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 trl==0.4.7"]},{"cell_type":"code","source":["!pip install git+https://github.com/huggingface/transformers"],"metadata":{"id":"IirgDdoCunNM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load a quantized Mistral-7B Model"],"metadata":{"id":"oof7clgqr_Vc"}},{"cell_type":"code","source":["import streamlit"],"metadata":{"id":"JhLwXuKlOmn4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Imports\n","import os\n","import torch\n","import nest_asyncio\n","\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    pipeline\n",")\n","from datasets import load_dataset\n","from peft import LoraConfig, PeftModel\n","\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain.document_transformers import Html2TextTransformer\n","from langchain.document_loaders import AsyncChromiumLoader\n","\n","from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n","from langchain.vectorstores import FAISS\n","\n","from langchain.prompts import PromptTemplate\n","from langchain.schema.runnable import RunnablePassthrough\n","from langchain.llms import HuggingFacePipeline\n","from langchain.chains import LLMChain"],"metadata":{"id":"o07bNFzbsfKN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#################################################################\n","# Tokenizer\n","#################################################################\n","\n","model_name='mistralai/Mistral-7B-Instruct-v0.1'\n","# model_name = 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\""],"metadata":{"id":"FqARtz0KrvdG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#################################################################\n","# bitsandbytes parameters\n","#################################################################\n","\n","# Activate 4-bit precision base model loading\n","use_4bit = True\n","\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","\n","# Activate nested quantization for 4-bit base models (double quantization)\n","use_nested_quant = False"],"metadata":{"id":"AOql1Fd1tgHN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up quantization config\n","#################################################################\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=use_nested_quant,\n",")\n","\n","# Check GPU compatibility with bfloat16\n","if compute_dtype == torch.float16 and use_4bit:\n","    major, _ = torch.cuda.get_device_capability()\n","    if major >= 8:\n","        print(\"=\" * 80)\n","        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","        print(\"=\" * 80)"],"metadata":{"id":"S2iYRRautkGN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#################################################################\n","# Load pre-trained config\n","#################################################################\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config\n",")"],"metadata":{"id":"PGlRciE9tnfk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Testing the model"],"metadata":{"id":"755gJ9kOvdzn"}},{"cell_type":"code","source":["inputs_not_chat = tokenizer.encode_plus(\"[INST] Tell me about fantasy football? [/INST]\", return_tensors=\"pt\")['input_ids'].to('cuda')\n","\n","generated_ids = model.generate(inputs_not_chat,\n","                               max_new_tokens=1000,\n","                               do_sample=True)\n","decoded = tokenizer.batch_decode(generated_ids)"],"metadata":{"id":"qBrsk72pvc3d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(decoded[0])"],"metadata":{"id":"MStOaXJCwZv0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Creating a RAG"],"metadata":{"id":"biMrpNEqwxwl"}},{"cell_type":"markdown","source":["### Create vector database"],"metadata":{"id":"_MiWqwyoxUhN"}},{"cell_type":"code","source":["from langchain.document_loaders import TextLoader\n","\n","FILE_PATH = 'ANEXO_14_Vol._1-29-47.txt'\n","with open(FILE_PATH, 'r+', encoding=\"utf-8\") as file:\n","    # Step 2: Read the contents\n","    doc = file.read()\n","\n","loader = TextLoader(FILE_PATH, encoding=\"utf-8\")\n","documents = loader.load()\n","\n","# Chunk text\n","text_splitter = CharacterTextSplitter(chunk_size=100,\n","                                      chunk_overlap=0)\n","\n","chunked_documents = text_splitter.split_documents(documents)\n","\n","# Load chunked documents into the FAISS index\n","db = FAISS.from_documents(chunked_documents,\n","                          HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n","\n","\n","# Connect query to FAISS index using a retriever\n","retriever = db.as_retriever(\n","    search_type=\"similarity\",\n","    search_kwargs={'k': 4}\n",")"],"metadata":{"id":"0U4PN2Rf0uoO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = 'What is the recommended longitudinal slope on a runway for code number 3?'\n","docs = db.similarity_search(query)\n","print(docs[1].page_content)"],"metadata":{"id":"HCILR-VQ0wmF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create LLM Chain"],"metadata":{"id":"km2VVgcU2Mrc"}},{"cell_type":"code","source":["text_generation_pipeline = pipeline(\n","    model=model,\n","    tokenizer=tokenizer,\n","    task=\"text-generation\",\n","    temperature=0.0,\n","    repetition_penalty=1.1,\n","    return_full_text=True,\n","    max_new_tokens=300,\n",")\n","\n","prompt_template = \"\"\"\n","### [INST]\n","Instruction: As an aerospace engineer, you are tasked with assisting users in addressing inquiries related to airport design. Here is context to help:\n","\n","{context}\n","\n","### QUESTION:\n","{question}\n","\n","[/INST]\n"," \"\"\"\n","\n","mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n","\n","# Create prompt from prompt template\n","prompt = PromptTemplate(\n","    input_variables=[\"context\", \"question\"],\n","    template=prompt_template,\n",")\n","\n","# Create llm chain\n","llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)"],"metadata":{"id":"lqg0RsJDyJlJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# llm_chain.invoke({\"context\":\"\",\n","#                   \"question\": \"What is the recommended longitudinal slope on a runway for code number 3?\"})"],"metadata":{"id":"V9FZrXwg3TcL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create RAG chain"],"metadata":{"id":"rsaGG_js3lyf"}},{"cell_type":"markdown","source":["import streamlit as st"],"metadata":{"id":"fJohG_Yo6HmS"}},{"cell_type":"code","source":["query = 'What is the recommended longitudinal slope on a runway for code number 3?'\n","retriever = db.as_retriever()\n","\n","rag_chain = (\n"," {\"context\": retriever, \"question\": RunnablePassthrough()}\n","    | llm_chain\n",")\n","\n","result = rag_chain.invoke(query)"],"metadata":{"id":"eNJV5oA43oyL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result"],"metadata":{"id":"zh5vw4UY31EV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result['context']"],"metadata":{"id":"zTsQafcB32JS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result['question']"],"metadata":{"id":"D1Br1FgO4sur"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result['text']"],"metadata":{"id":"0__e7hB94cor"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rag_chain.invoke('What are the recommendations for the radio altimeter operating area?')['text']"],"metadata":{"id":"QsharbdG4hnj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"k7nh7mujOb1h"},"execution_count":null,"outputs":[]}]}